{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_01_Quiz.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaMS/Convolutional-Neural-Networks-in-TensorFlow/blob/master/Week_04_Quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b4A0Bqfsgxq",
        "colab_type": "text"
      },
      "source": [
        "# Week 3 Quiz\n",
        "1. If I put a dropout parameter of 0.2, how many nodes will I lose?\n",
        "\n",
        "> 20% of them\n",
        "\n",
        "2. Why is transfer learning useful?\n",
        "\n",
        "\n",
        "> Because I can use the features that were learned from large datasets that I may not have access to?\n",
        "\n",
        "\n",
        "3.How did you lock or freeze a layer from retraining?\n",
        "\n",
        "> layer.trainable=false\n",
        "4.How do you change the number of classes the model can classify when using transfer learning? (i.e. the original model handled 1000 classes, but yours handles just 2)\n",
        "\n",
        "\n",
        "> When you add your DNN at the bottom of the network, you specify your output layer with the number of classes you want\n",
        "\n",
        "\n",
        "5.Can you use Image Augmentation with Transfer Learning Models?\n",
        "\n",
        "> Yes, because you are adding new layers at the bottom of the network, and you can use image augmentation when training these\n",
        "\n",
        "6.Why do dropouts help avoid overfitting?\n",
        "\n",
        "> Because neighbor neurons can have similar weights, and thus can shew the final training\n",
        "\n",
        "\n",
        "7.What would the symptom of a Dropout rate being set too high?\n",
        "\n",
        "\n",
        "> The network would lose specialization to the effect that it would be inefficient or ineffective at learning, driving accuracy down\n",
        "\n",
        "8. Which is the correct line of code for adding Dropout of 20% of neurons using TensorFlow\n",
        "\n",
        "\n",
        "> tf.keras.layers.Dropout(0.2)\n"
      ]
    }
  ]
}